---
title: "Assignment 1"
author: David Krütli, Serhat Nergiz, Ibrahim Yaman, Mohamed Ramadan<br> 
date: "`r Sys.time()`"
output:
  github_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Business Understanding

Text **to follow** here.

# Setup
## Load Required Packages

First of all we are going to load the packages required for this project.

```{r message = FALSE, warning = FALSE}
#install.packages("readr")
#install.packages("tidyverse")
#install.packages("formattable")
library(car)
library(corrplot)
library(dbplyr)
library(formattable)
library(Hmisc)
library(lmtest)
library(openxlsx)
library(readr)
library(tidyverse)

```


## Load Data

Next we are going to load the data from the CSV file "LCdata.csv" and display the first row to get a rough idea about the data.

```{r message = FALSE, warning = FALSE}
creditData <- read_delim("/Users/serhat/Downloads/LCdata.csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
knitr::kable(head(creditData, 1))
```

## Data removal
The decision to drop certain variables from the dataset for predicting the interest rate of new loan applications is based on their unavailability or irrelevance at the time of prediction. First of all we are going to drop these variables:

```{r message = FALSE, warning = FALSE}
creditData2 <- subset(creditData,select=-c(collection_recovery_fee,
                                                       installment,
                                                       issue_d,
                                                       last_pymnt_amnt,
                                                       last_pymnt_d,
                                                       loan_status,
                                                       next_pymnt_d,
                                                       out_prncp,
                                                       out_prncp_inv,
                                                       pymnt_plan,
                                                       recoveries,
                                                       term,
                                                       total_pymnt,
                                                       total_pymnt_inv,
                                                       total_rec_int,
                                                       total_rec_late_fee,
                                                       total_rec_prncp))
```


Next we are removing all variables with more than 200'000 blank values.

```{r message = FALSE, warning = FALSE}
creditData2<- subset(creditData2,select=-c(annual_inc_joint,
                                                       desc,
                                                       dti_joint,
                                                       mths_since_last_delinq,
                                                       mths_since_last_major_derog,
                                                       mths_since_last_record,
                                                       open_acc_6m,
                                                       open_il_6m,
                                                       open_il_12m,
                                                       open_il_24m,
                                                       mths_since_rcnt_il,
                                                       total_bal_il,
                                                       il_util,
                                                       open_rv_12m,
                                                       open_rv_24m,
                                                       max_bal_bc,
                                                       all_util,
                                                       total_rev_hi_lim,
                                                       inq_fi,
                                                       total_cu_tl,
                                                       inq_last_12m))
```

We will also drop the variable url because with 798641 levels it is not going to say anything about our data. The policy code is also not really useful to predict the interest rate, because it only has the numeric value 1.

```{r message = FALSE, warning = FALSE}
creditData2<- subset(creditData2,select=-c(policy_code,
                                                       url))
```
# Explore Data

## Attribute revol_util

Display a summary of the attribute **revol_util**

```{r message = FALSE, warning = FALSE}
summary(creditData2$revol_util)
```

As we can see, this attribute has 454 blanks which we are going to remove entirely from the dataset.

```{r message = FALSE, warning = FALSE}
creditData2<- filter(creditData, ! is.na(revol_util))
creditData2$title <- tolower(creditData2$title)
```

## Attribute title
Display a summary of the attribute **revol_util**
```{r message = FALSE, warning = FALSE}
summary(creditData2$title)
```

It is visible, that we have 56143 levels and this is way to much. Let's see which of them are occuring the most.

```{r message = FALSE, warning = FALSE}

max(table(creditData2$title))
HäufigkeitTitle <- round(prop.table(table(creditData2title))*100,2)

tabelle <- cbind(HäufigkeitTitle)

kategorie_haeufigkeit <- table(creditData2$title)

sortierte_haeufigkeit <- sort(kategorie_haeufigkeit, decreasing = TRUE)

top30 <- head(sortierte_haeufigkeit, 30)

top30_namen <- names(top30)

creditData2$title <- ifelse(creditData2$title %in% top30_namen, creditData2$title, 'others')

table(creditData2$title)
creditData2$title <- factor("title")
```

We cann see that debt consolidation and credit card refinancing are dominating the categories with over 67% and many of the categories are redundant due to 
upper and lower cases. So we are making every value in lower cases and take the top 30 categories for the predicition.The rest will be sum up as "others"

## Attribute addr_state

Display a summary of the attribute **addr_state**:

```{r message = FALSE, warning = FALSE}
summary(creditData$addr_state)
```

Plot the distribution of the attribute **addr_state**:

```{r message = FALSE, warning = FALSE}
plot(table(creditData$addr_state))
```

Build a contingency table of the counts at each combination of factor levels.

```{r message = FALSE, warning = FALSE}

table(creditData$addr_state)
```

**Conclusion**:

We should keep this attribute in the dataset for the moment. The interest rate could depend on the area where the borrower lives. This attribute needs to be converted into a categorical value (factor).

## Attribute annual_inc

Display a summary of the attribute **annual_inc**:

```{r message = FALSE, warning = FALSE}
summary(creditData$annual_inc)
```

Plot a historgram for **annual_inc**, where the annual income is higher than 500'000:

```{r message = FALSE, warning = FALSE}
hist((filter(creditData, annual_inc > 500000)$annual_inc), breaks = 1000)
```



As can be seen in the histogram, there are a few very high values, so we want to see if these are errors in our dataset. Therefore we are going to list the **emp_title** and **verification_status** for the top 50 values.

```{r message = FALSE, warning = FALSE}

select(head(creditData[order(creditData$annual_inc, decreasing = TRUE), ], 50), annual_inc, emp_title, verification_status)
```

Looks like there are indeed some unrealistic values in our dataset: A nurse with an annual income of USD 9.5 million or a commercial driver with USD 8.9 million seems to be way above the expected income, although source was verified.

**Conclusion**:

We should keep this attribute in the dataset for the moment. The interest rate would probably depend on the borrower's annual income, as this tells us something about the borrower's creditworthiness. The 4 records with NA for the annual income should be removed. We should also consider filling the values above a certain threshold (e.g. 800'000) with the median (i.e., 65'000).

## ...