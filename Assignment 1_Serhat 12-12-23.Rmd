---
title: "Assignment 1"
author: "David Krütli, Serhat Nergiz, Ibrahim Yaman, Mohamed Ramadan<br>"
date: "`r Sys.time()`"
output:
  pdf_document:
    toc: yes
    number_sections: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Business Understanding

Business Understanding

Introduction

The Fintech sector is constantly evolving due to the rapid enhancement within the level of technology and newly emerged business models. The project that we are working on involves a company that operates in that sphere and the need to adapt to the constant evolution of that sector is essential for their sustainability. (Giglio, 2021)

The company is called LendingClub and it’s a company that provides a service in a peer-to-peer form “P2P”. The company has a lending platform that allows people who need to borrow money with people who want to invest their money. The LendingClub allows that form of exposure between the two parties to be initiated in a convenient way that does give big advantages over the traditional ways of borrowing or lending money. The technology allows LendingClub to offer the service online which in return minimizes their running costs and in return, the clients get better interest rates with the liberty of applying for loans through the comfort of their home by making an application. (Frankel, 2021)

The goal of the company is to keep its business model active so that it can have a sustainable business; thus being a middle company between the two parties is how they generate profit. The company needs to have enough applications and contracts to help its borrowers get the money they need and their lenders to profit from their investments. This requires creating a piece of detailed information about the borrower's capabilities to acquire the loan and it is more flexible than traditional ways as it doesn’t require a long time if all the information from the borrower is handled accurately through some elements such as their credit score, credit history, etc. (Giglio, 2021)

As well as the LendingClub offers another service in which they have a secondary market for investors to sell their commitment to a certain loan to another interested investor. LendingClub calls it “Notes”, the notes are a portion of the loan that is divided into many notes, and then investors come and invest in all the loan notes or just a portion of it; which allows investors to have the ability to diversify their portfolio based on their investment needs.(Chang et al., 2022)

Finally, LendingClub has decided that to keep up with the Fintech industry they have to upgrade their business model from peer to peer lending platform to a business model of online banking to allow themselves to be more sustainable in terms of their growth in the market. The company offers a competitive interest rates, digital platform with diverse interest parties whether borrowers or investors, transparent fees, and flexible loan agreements. However this shouldn’t affect any existing notes that LendingClub clients own, nor it shouldn’t affect the institutional investors who are doing business with the company.(Chang et al., 2022)

Methodology

LendingClub is a peer-to-peer lending platform that connects borrowers with individual investors. Here's how it generally works:(Chang et al., 2022)

As mentioned previously, the platform connects the two interested parties in a specific setting. First, the borrower applies for loans through the LendingClub platform and as a company gets a notification and process starts to evaluate the creditworthiness of the applicants using algorithms based on the data that they provide whilst assigning the applicants to a credit grade.

Second Now that acts as an asset and gets promoted on the platform waiting for the prospective investors who are looking to earn profits to evaluate the details of the approved loans and their purpose and make a decision to invest.

Third the investor can invest as little as $25 in each loan which gives them more flexible options for portfolio diversification, and borrowers will receive their loan while agreeing to repay their loan over a fixed period which is usually between 3 to 5 years.

From that stage, the borrowers are obliged to make monthly payments including the interest and original over their loan agreement. The inventors on the other hand receive a portion of these payments subject to their investment within that specific loan. As a rule of thumb, the company decides the interest rates which are based on the credit risk of the borrower, while investors will have the opportunity to have a higher return compared to other investment options thus creating a risk that also borrowers may default over their loans. Also to provide some sort of security for investors they offered them to sell their obligations for a specific type of loan to other investors who might be interested as a form of an exit strategy through a secondhand market of “Notes”. (LendingClub, 2023)

The company evaluates the potential borrowers based on a set of criteria to assess their creditworthiness and evaluate the terms of the loan. While the specific criteria may vary, the following are common factors to consider when choosing customers: Credit Score, Credit History, Debt-to-Income Ratio, Employment and Income, Loan Purpose, Length of Employment, Verification of Information, and Public Records that could show any sort of bankruptcies, tax issues, or other negative items in public records may impact the borrower's eligibility and loan agreements.(Chang et al., 2022)

The interest rate on a personal loan can vary based on several factors, including the borrower's creditworthiness, the lender's policies, loan amount, loan term, and prevailing market conditions. A good personal loan interest rate often falls within the range of 6% to 36%. (Pál, 2023)

1. Excellent Credit (720 and above):
· Borrowers with excellent credit often qualify for the lowest interest rates, typically in the single digits or low teens.

2. Good Credit (660 - 719):
· Those with good credit may still receive favorable rates, ranging from the mid to high single digits up to the low twenties.

3. Fair Credit (620 - 659):
· Individuals with fair credit might face higher interest rates, often in the high teens to mid-twenties.

4. Poor Credit (below 620):
· Borrowers with poor credit may have limited options and could face interest rates toward the upper end of the scale, potentially exceeding 30%.

The company also uses the Debt-to-Income (DTI) ratio to assess an individual's or a household's ability to manage debt in relation to their income. It is expressed as a percentage and is calculated by dividing total monthly debt payments by gross monthly income. The DTI ratio provides investors and financial institutions with insights into a borrower's financial health and their capacity to take on additional debt. (Team, 2021)

1. Calculation:

DTI Ratio = (Total Monthly Debt Payments / Gross Monthly Income) x 100

Total Monthly Debt Payments: This includes all recurring monthly debt obligations such as mortgage or rent payments, car loans, credit card payments, student loans, and any other debt obligations.

Gross Monthly Income: This is the total income before deducting taxes and other withholdings.

The resulting DTI ratio is expressed as a percentage. For example, a DTI ratio of 20% means that 20% of gross monthly income is allocated to debt payments. There are two types of DTI, Front-End DTI which considers only housing-related expenses (mortgage or rent, property taxes, and homeowner's insurance). And Back-End DTI which considers all debt obligations, including housing-related expenses and other debts like credit cards and loans.

Lenders often have guidelines for acceptable DTI ratios. Lower DTI ratios are considered more favorable, which indicates that a smaller percentage of income is

allocated to debt. As well as a lower DTI ratio may enhance a borrower's ability to qualify for loans and secure more favorable interest rates. While a higher DTI ratio may signal higher financial risk to lenders, potentially impacting the approval for new credit. (Srivastav, 2021)

The company also uses a revolving credit which is a credit arrangement without a predetermined number of payments, allowing the borrower to utilize the available credit up to a set limit repeatedly. In contrast to installment credit, where a fixed sum is borrowed and repaid in consistent installments, revolving credit enables borrowers to access the credit line, make payments, and subsequently utilize the credit again. Ex. Credit cards are a common form of revolving credit. Cardholders can make purchases up to their credit limit, repay the balance, and then reuse the available credit. Revolving credit provides flexibility and convenience but requires responsible management to avoid accumulating high levels of debt. (Axelton, 2023)

The Key difference between revolving and installment credit is that both types of credit have their advantages and considerations. Installment credit is suitable for specific, one-time expenses while revolving credit offers ongoing access to funds for various purposes. Individuals often use a combination of both types of credit based on their financial needs and goals. (Axelton, 2023)

The company also uses the APR “Annual Percentage Rate” which is a number that expresses the total cost accumulated from lending money. It acts as an interest rate that is applied once the borrower doesn’t fulfill his balance obligation to the lender which is an interest rate that excludes all the other costs of the credit card. Another form of APR is on installment loans which takes into account the APR, fees, and any other form of costs. Both have different forms of calculation thus it varies from credit card to installment loan. (Luthi, 2023)

All the elements of information provided previously affect the process of lending money so if a company has to build algorithms that approve a loan and generate a profit over it; it should be managed carefully and constantly to allow the process to flow smoothly with no losses either for the borrower or lenders.

We will aim to study the data of LendingClub and implement several algorithms models to help us achieve a good understanding of Lending club business model and to check possible ways to achieve better results after investigating their current data. We will try to construct that by using several machine learning and artificial neural network (ANN) methods including linear regression, decision trees, and random forests. We also aim as part of our 2nd part of the Assignment to create a neural network for exploring several hyper parameter settings for each method by performing a grid search and cross-validation to get the most suitable credit-scoring model in terms of training time and test performance.

Bibliography

1. Axelton, K. (2023, Jan 06). What Is Revolving Credit? Retrieved from experian: https://www.experian.com/blogs/ask-experian/what-is-revolving-credit/
2. Frankel, M. (2021, July 21). The Ascent. Retrieved from Fool: https://www.fool.com/the-ascent/personal-loans/articles/lendingclub-ending-its-p2p-lending-platform-now-what/
3. Giglio, F. (2021). Fintech: A Literature Review. European Research Studies Journal, 600-627.
4. LendingClub. (2023). Does LendingClub Still Issue New Notes? Retrieved from LendingClub: https://lendingclub.zendesk.com/hc/en-us/articles/360050574891-Does-LendingClub-Still-Issue-New-Notes-
5. Luthi, B. (2023, November 14). What Is an Installment Loan? Retrieved from experian: https://www.experian.com/blogs/ask-experian/what-is-installment-loan/
6. Pál, T. (2023, June 05). Interest Rate Calculator. Retrieved from Omni Calculator: https://www.omnicalculator.com/finance/interest-rate
7. Srivastav, A. K. (2021). Debt to Income Ratio (DTI). Retrieved from wallstreetmojo: https://www.wallstreetmojo.com/debt-to-income-ratio/
8. Team, C. (2021). Debt-to-Income Ratio. Retrieved from corporatefinanceinstitute: https://corporatefinanceinstitute.com/resources/commercial-lending/debt-to-income-ratio/
9. Chang, A.-H., Yang, L.-K., Tsaih, R.-H., & Lin, S.-K. (2022). Machine learning and artificial neural networks to construct P2P lending credit-scoring model: A case using Lending Club data. Quantitative Finance and Economics, 6(2), 303–325. https://doi.org/10.3934/QFE.2022013

# Setup
## Load Required Packages

First of all we are going to load the packages required for this project.

```{r message = FALSE, warning = FALSE}
#install.packages("readr")
#install.packages("tidyverse")
#install.packages("car")
#install.packages("corrplot")
#install.packages("dbplyr")
#install.packages("formattable")
#install.packages("Hmisc")
#install.packages("lmtest")
#install.packages("openxlsx")
#install.packages("readxl")
#install.packages("ggplot2")
#install.packages("tree")

library(car)
library(corrplot)
library(dbplyr)
library(formattable)
library(Hmisc)
library(lmtest)
library(openxlsx)
library(readr)
library(tidyverse)
library(readxl)
library(ggplot2)
library(tree)
```


## Load Data

Next we are going to load the data from the CSV file "LCdata.csv" and display the first row to get a rough idea about the data.

```{r message = FALSE, warning = FALSE}
creditData <- read_delim("/Users/serhat/Downloads/LCdata.csv", delim = ";", escape_double = FALSE, trim_ws = TRUE)
knitr::kable(head(creditData, 1))
```

## Structure of the Dataset

```{r message = FALSE, warning = FALSE, echo=FALSE, results = 'asis'}
cat(paste0("The dataset has **", nrow(creditData), "** rows and **", ncol(creditData), "** columns."))
```

## Data removal
The decision to drop certain variables from the dataset for predicting the interest rate of new loan applications is based on their unavailability or irrelevance at the time of prediction. First of all we are going to drop these variables:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditData,select=-c(collection_recovery_fee,
                                                       installment,
                                                       issue_d,
                                                       last_pymnt_amnt,
                                                       last_pymnt_d,
                                                       loan_status,
                                                       next_pymnt_d,
                                                       out_prncp,
                                                       out_prncp_inv,
                                                       pymnt_plan,
                                                       recoveries,
                                                       term,
                                                       total_pymnt,
                                                       total_pymnt_inv,
                                                       total_rec_int,
                                                       total_rec_late_fee,
                                                       total_rec_prncp))
```


Next we are removing all variables with more than 200'000 blank values.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed,select=-c(annual_inc_joint,
                                                       desc,
                                                       dti_joint, 
                                                       mths_since_last_delinq, 
                                                       mths_since_last_major_derog, 
                                                       mths_since_last_record,
                                                       open_acc_6m,
                                                       open_il_6m,
                                                       open_il_12m,
                                                       open_il_24m,
                                                       mths_since_rcnt_il,
                                                       total_bal_il,
                                                       il_util,
                                                       open_rv_12m,
                                                       open_rv_24m,
                                                       max_bal_bc,
                                                       all_util,
                                                       total_rev_hi_lim,
                                                       inq_fi,
                                                       total_cu_tl,
                                                       inq_last_12m,
                                                       id,
                                                       member_id,
                                                       emp_title))
```

We will also drop the variable url because with 798641 levels it is not going to say anything about our data. The policy code is also not really useful to predict the interest rate, because it only has the numeric value 1. Furthermore, the variable verified_status_joint will be dropped since there are only NA's in there.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed,select=-c(policy_code,url,verification_status_joint))
```
# Explore Data

## Attribute revol_util

Display a summary of the attribute **revol_util**

```{r message = FALSE, warning = FALSE}
summary(creditData$revol_util)
```

As we can see, this attribute has 454 blanks which we are going to remove entirely from the dataset.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- filter(creditDataProcessed, ! is.na(revol_util))
```

We should also check if there are any outliers:
```{r message = FALSE, warning = FALSE}
plot(creditData$revol_util, creditData$int_rate,
     xlab = "revol_util", 
     ylab = "int_rate",
     main = "Scatter plot of revol_util",
     pch = 20,
     col = "blue")
```
It seems to have some outliers. We do remove values over 200.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, revol_util <= 200)
```

## Attribute title
Display the amount of levels of the attribute **title**
```{r message = FALSE, warning = FALSE}
length(unique(creditData$title))
```

How many do we have if we lower all the cases?
```{r message = FALSE, warning = FALSE}
creditData$title <- tolower(creditData$title)
length(unique(creditData$title))
```
8818 less than with upper and lower cases

There are still special characters in the variables. Let's remove them:
```{r message = FALSE, warning = FALSE}
creditData$title <- gsub("[^A-Za-z0-9 ]", "", creditData$title)
length(unique(creditData$title))
```

1477 less.
We can also remove the numbers.
```{r message = FALSE, warning = FALSE}
creditData$title <- gsub("[^A-Za-z ]", "", creditData$title)
length(unique(creditData$title))
```

Again, 1209 less.

Let's see which of them are occuring the most.

```{r message = FALSE, warning = FALSE}
max(table(creditData$title))
FrequencyTitle <- round(prop.table(table(creditData$title))*100,2)
table <- cbind(FrequencyTitle)
sorted_table <- table[order(table, decreasing = TRUE), ]

# Now, get the top 20 rows
top_20_rows <- head(sorted_table, 20)

# Display the top 20 rows
print(top_20_rows)
```

In the table we can see that debt consolidation and credit card refinancing are dominating the categories with over 67%. So we are taking the top 5 categories for the predicition. The rest will be sum up as "others"


```{r message = FALSE, warning = FALSE}
category_frequency <- table(creditDataProcessed$title)
sorted_frequency <- sort(category_frequency, decreasing = TRUE)
top5 <- head(sorted_frequency, 5)
top5_names <- names(top5)
creditDataProcessed$title <- ifelse(creditDataProcessed$title %in% top5_names, creditDataProcessed$title, 'Other')
```

Now we can see which categories are left:
```{r message = FALSE, warning = FALSE}
table(creditDataProcessed$title)
```

## Attribute zip_code

Let us first have a look at the variable:
```{r message = FALSE, warning = FALSE}
summary(creditData$zip_code)
head(creditData$zip_code)
```
Let's have a more in depth view on the data:

```{r message = FALSE, warning = FALSE}
tab <- table(creditData$zip_code, creditData$addr_state)
print(tab)
```

As we can see with the table function, some of the zip_code are occuring in multiple states. Therefore, it makes sense to combine the zip_code and addr_state variable to create truely unique values. This way it could give us a better prediction for the interest rate.

```{r message = FALSE, warning = FALSE}
creditDataProcessed$zip_code <- substr(creditDataProcessed$zip_code, 1, 3)
creditDataProcessed$Combo <- paste0(creditDataProcessed$zip_code, creditDataProcessed$addr_state)
```


## Attribute acc_now_deling
Let us first have a look at the variable:
```{r message = FALSE, warning = FALSE}
summary(creditData$acc_now_delinq)
```

More than 75% of the data seems to have the value 0. We can keep it as it is for further analysis. But we can check how many values are higher than zero to understand the data better.

```{r message = FALSE, warning = FALSE}
Higher0 <- sum(creditDataProcessed$acc_now_delinq > 0, na.rm = TRUE)
print(Higher0)
```

We should also check if there are any outliers:
```{r message = FALSE, warning = FALSE}
plot(creditData$acc_now_delinq, creditData$int_rate,
     xlab = "acc_now_delinq", 
     ylab = "int_rate",
     main = "Scatter plot of acc_now_delinq",
     pch = 20,
     col = "blue")
```
There seems to have some outliers, so we remove these.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, acc_now_delinq <= 8)
```

## Attribute tot_coll_amt
Now we can have a look at the attribute tot_coll_amt.
```{r message = FALSE, warning = FALSE}
summary(creditData$tot_coll_amt)
```
There are many NA's with 63276 but we don't want to drop the variable since it is less than 10% of the whole data set. Let's see how many values are exactly 0.

```{r message = FALSE, warning = FALSE}
zero_in_column <- sum(creditData$tot_coll_amt == 0, na.rm = TRUE)
print(zero_in_column)
```

630268 cells with the value 0. It makes absolutely sense to replace the NA's with the median, which would be 0.

```{r message = FALSE, warning = FALSE}
median_tot_coll_amt <- median(creditDataProcessed$tot_coll_amt, na.rm = TRUE)

creditDataProcessed$tot_coll_amt[is.na(creditDataProcessed$tot_coll_amt)] <- median_tot_coll_amt
```

We should also check if there are any outliers:
```{r message = FALSE, warning = FALSE}
plot(creditData$tot_coll_amt, creditData$int_rate,
     xlab = "tot_coll_amt", 
     ylab = "int_rate",
     main = "Scatter plot of tot_coll_amt",
     pch = 20,
     col = "blue")
```
There is an outlier, so we are removing it.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, tot_coll_amt <= 2000000)
```

## Attribute tot_cur_bal

Now we can have a look at the attribute tot_cur_bal.
```{r message = FALSE, warning = FALSE}
summary(creditData$tot_cur_bal)
```
There are many NA's with 63276 but we don't want to drop the variable since it is less than 10% of the whole data set. Let's see how many values are exactly 0.

```{r message = FALSE, warning = FALSE}
zero_in_column <- sum(creditData$tot_cur_bal == 0, na.rm = TRUE)
print(zero_in_column)
```

Only 114 cells with the value 0. It makes absolutely sense to replace this time the NA's with the mean, which would be 139508. The distribution between the value seems also alright.

```{r message = FALSE, warning = FALSE}
mean_tot_cur_bal <- mean(creditDataProcessed$tot_cur_bal, na.rm = TRUE)

creditDataProcessed$tot_cur_bal[is.na(creditDataProcessed$tot_cur_bal)] <- mean_tot_cur_bal
```

We should also check if there are any outliers:
```{r message = FALSE, warning = FALSE}
plot(creditData$tot_cur_bal, creditData$int_rate,
     xlab = "tot_cur_bal", 
     ylab = "int_rate",
     main = "Scatter plot of tot_cur_bal",
     pch = 20,
     col = "blue")
```
There is an outlier, so we are removing it.

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, tot_cur_bal <= 4000000)
```

## Attribute addr_state

Display a summary of the attribute **addr_state**:

```{r message = FALSE, warning = FALSE}
summary(creditData$addr_state)
```

Plot the distribution of the attribute **addr_state**:

```{r message = FALSE, warning = FALSE}
plot(table(creditData$addr_state))
```

Build a contingency table of the counts at each combination of factor levels.

```{r message = FALSE, warning = FALSE}
table(creditData$addr_state)
```

**Conclusion**:

We should keep this attribute in the dataset for the moment. The interest rate could depend on the area where the borrower lives. This attribute needs to be converted into a categorical value (factor).

## Attribute annual_inc

Display a summary of the attribute **annual_inc**:

```{r message = FALSE, warning = FALSE}
summary(creditData$annual_inc)
```

Plot a historgram for **annual_inc**, where the annual income is higher than 500'000:

```{r message = FALSE, warning = FALSE}
hist((filter(creditData, annual_inc > 500000)$annual_inc), breaks = 1000)
```



As can be seen in the histogram, there are a few very high values, so we want to see if these are errors in our dataset. Therefore we are going to list the **emp_title** and **verification_status** for the top 50 values.

```{r message = FALSE, warning = FALSE}

select(head(creditData[order(creditData$annual_inc, decreasing = TRUE), ], 50), annual_inc, emp_title, verification_status)
```

Looks like there are indeed some unrealistic values in our dataset: A nurse with an annual income of USD 9.5 million or a commercial driver with USD 8.9 million seems to be way above the expected income, although source was verified.

**Conclusion**:

We should keep this attribute in the dataset for the moment. The interest rate would probably depend on the borrower's annual income, as this tells us something about the borrower's creditworthiness. The 4 records with NA for the annual income should be removed. We should also consider filling the values above a certain threshold (e.g. 800'000) with the median (i.e., 65'000).

Remove NAs and overwrite values > 800'000 with 65'000:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, is.na(creditDataProcessed$annual_inc) != 1)
creditDataProcessed$annual_inc <- if_else(creditDataProcessed$annual_inc > 800000, 65000, creditDataProcessed$annual_inc)
```

## Attribute application_type

Display a summary of the attribute **application_type**:

```{r message = FALSE, warning = FALSE}
summary(creditData$application_type)
```

Plot the distribution of the attribute **application_type**:

```{r message = FALSE, warning = FALSE}
plot(table(creditData$application_type))
```

Build a contingency table of the counts at each combination of factor levels.

```{r message = FALSE, warning = FALSE}
table(creditData$application_type)
```

**Conclusion**:

The vast majority of applications are INDIVIDUAL (798181). Only 460 are JOINT.


## Attribute verification_status

Display a summary of the attribute **verification_status**:

```{r message = FALSE, warning = FALSE}
count(creditData[is.na(creditData$verification_status), ])
class(creditData$verification_status)
```

## Attribute open_acc

Display the attribute open_acc
```{r message = FALSE, warning = FALSE}
summary(creditData$open_acc)
```

Display the class of the attribute open_acc:

```{r message = FALSE, warning = FALSE}
class(creditData$open_acc)
```
Assign the median value into the NAs

```{r message = FALSE, warning = FALSE}
med_open_acc <- median(creditData$open_acc, na.rm = TRUE)
med_open_acc
creditDataProcessed[is.na(creditDataProcessed$open_acc), "open_acc"] <- med_open_acc

```


## Attribute last_credit_pull_d

Display the unique values of last_credit_pull_d
```{r message = FALSE, warning = FALSE}
unique(creditData$last_credit_pull_d)
```
Display the class

```{r message = FALSE, warning = FALSE}

class(creditData$last_credit_pull_d)
```

Remove the blanks

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- creditDataProcessed[!is.na(creditDataProcessed$last_credit_pull_d), ]
creditDataProcessed
```

## Attribute pub_rec

Assign the median into the missing values

```{r message = FALSE, warning = FALSE}
med_pub_rec <- median(creditDataProcessed$pub_rec, na.rm = TRUE)
med_pub_rec
creditDataProcessed[is.na(creditDataProcessed$pub_rec), "pub_rec"] <- med_pub_rec

```
## Attribute purpose

Factorize the character
```{r message = FALSE, warning = FALSE}
#creditDataProcessed$purpose <- as.numeric(creditDataProcessed$purpose)
class(creditDataProcessed$purpose)

```

## Attribute collections_12_mths_ex_med

Display a summary of the attribute **collections_12_mths_ex_med**:

```{r message = FALSE, warning = FALSE}
summary(creditData$collections_12_mths_ex_med)
```

As shown, it's a numerical value with 126 NAs and we assume that the number of collections is recorded as integer so we want to display a frequency table:

```{r message = FALSE, warning = FALSE}
table(creditData$collections_12_mths_ex_med)
```
**Conclusion**
As expected the number of collections decreases and the majority of cases have not had a collection. The 126 NAs should be removed.

Removing the 126 NAs:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, is.na(creditDataProcessed$collections_12_mths_ex_med) != 1)
```


## Attribute delinq_2yrs

Display a summary for the atrribute **delinq_2yrs**

```{r message = FALSE, warning = FALSE}
summary(creditData$delinq_2yrs)
```
As shown, it's a numerical value with 25 NAs and we assume that the number of collections is recorded as integer so we want to display a frequency table:

```{r message = FALSE, warning = FALSE}
table(creditData$delinq_2yrs)
```
**Conclusion**
Only integers (whole numbers) are used. Most of the records (645151) did not have a 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years. Densitiy decreases the higher the number of  incidences of delinquency.

Removing the 25 NAs:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, is.na(creditDataProcessed$delinq_2yrs) != 1)
```


## Attribute dti

Display a summary for the atrribute **dti**

```{r message = FALSE, warning = FALSE}
summary(creditData$dti)
```
Probably some outliers in the data:

```{r message = FALSE, warning = FALSE}
plot(creditData$dti)
```

Let's display the top 100 values for dti:

```{r message = FALSE, warning = FALSE}
head((creditData %>% arrange(desc(creditData$dti)))$dti, 100)
```

**Conclusion**
Should probably remove all the records with values equal to or above 100 for dti:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, creditDataProcessed$dti < 100)
```

## Attribute earliest_cr_line

Let's see the structure of the attribute **earliest_cr_line**:

```{r message = FALSE, warning = FALSE}
str(creditData$earliest_cr_line)
```
Are there any NAs?

```{r message = FALSE, warning = FALSE}
sum(is.na(creditData$earliest_cr_line))
```
Reove the 25 NAs:

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- subset(creditDataProcessed, is.na(creditDataProcessed$earliest_cr_line) != 1)

str(creditDataProcessed$earliest_cr_line)
```

## Attribute emp_length

Let's see the structure of the attribute **emp_length**:

```{r message = FALSE, warning = FALSE}
str(creditData$emp_length)
```
Are there any NAs?

```{r message = FALSE, warning = FALSE}
sum(is.na(creditData$emp_length))
```

Let's see a frequency table for the attribute:

```{r message = FALSE, warning = FALSE}
plot(table(creditDataProcessed$emp_length))
```

Convert the attribute into a factor:

```{r message = FALSE, warning = FALSE}
creditDataProcessed$emp_length <- as.factor(creditDataProcessed$emp_length)
str(creditDataProcessed$emp_length)
```

## Attribute funded_amnt

Let's get a summary:

```{r message = FALSE, warning = FALSE}
summary(creditData$funded_amnt)
```
Let's plot a histogram:

```{r message = FALSE, warning = FALSE}
hist(creditData$funded_amnt)
```



## Attribute funded_amnt_inv

Display the summary of funded_amnt_inv

```{r message = FALSE, warning = FALSE}
summary(creditData$funded_amnt_inv)
```
Display the histogram of the feature to see distribution of values

```{r message = FALSE, warning = FALSE}
hist(creditData$funded_amnt_inv)
```

## Attribute home_ownership

```{r message = FALSE, warning = FALSE}
summary(creditData$home_ownership)


plot(table(creditDataProcessed$home_ownership))


str(creditDataProcessed$home_ownership)
```



## Attribute initial_list_status

Let's see how many different status we have

```{r message = FALSE, warning = FALSE}
unique(creditData$initial_list_status)
```


```{r message = FALSE, warning = FALSE}
summary(creditData$initial_list_status)
```


```{r message = FALSE, warning = FALSE}
print(creditDataProcessed$initial_list_status)
```

Display all feature classes that we have chosen

```{r message = FALSE, warning = FALSE}
variable_classes <- sapply(creditDataProcessed, class)
print(variable_classes)
```

```{r message = FALSE, warning = FALSE}
creditDataProcessed <- creditDataProcessed[!is.na(creditDataProcessed$revol_bal), ]
```


# Predicition
```{r message = FALSE, warning = FALSE}

##############   3. Validation Set Approach ##########################

set.seed(1)

#Training and test data set
train_indx <- sample(1:nrow(creditDataProcessed), 0.7*nrow(creditDataProcessed))
creditDataProcessed.train <- creditDataProcessed[train_indx,]
creditDataProcessed.test <- creditDataProcessed[-train_indx,]

#building the tree
tree.InterestRate <- tree(int_rate ~ . , data = creditDataProcessed.train)

#Summary of the tree
summary(tree.InterestRate)

#Visualization of the tree
plot(tree.InterestRate)
text(tree.InterestRate, cex=0.75)

#Textual from of tree
tree.InterestRate

#Training error (RMSE)
#4.15 compared to mean, which is 13.2449 kind of high
(tree.InterestRate.train.RMSE <- sqrt(summary(tree.InterestRate)$dev/summary(tree.InterestRate)$df))
```

```{r message = FALSE, warning = FALSE}
# Check if this is plausible by visually comparing the predicted Salaries to the true Salaries:
InterestRatecreditDataPreprocessed.train <- creditDataProcessed.train$int_rate # True Salaries on training data
tree.InterestRate.predict <- predict(tree.InterestRate, creditDataProcessed.train) # "Predicted" Salaries on on training data
plot(InterestRatecreditDataPreprocessed.train, tree.InterestRate.predict)
abline (0 ,1) 
#   abline (0 ,1) draws the function f(x) = x (intercept 0, slope 1).
#   For a perfect prediction, all points would lie in this line. 
#   We can see that the predicted values indeed deviate a lot from the true values. Thus, the high training MSE is plausible.
```
# Test error (RMSE)
```{r message = FALSE, warning = FALSE}
tree.InterestRate.predict <- predict(tree.InterestRate, creditDataProcessed.test) # To calculate it, we first need to predict the test data: As always, use the the generic function predict().
(tree.InterestRate.test.dev <- sum((tree.InterestRate.predict - creditDataProcessed.test$int_rate)^2)) # Calculate the deviance (RSS) 
(tree.InterestRate.test.RMSE <- sqrt(tree.InterestRate.test.dev/summary(tree.InterestRate)$df)) # Calculate the RMSE (divide by degrees of freedom and take the square root).
#2.717 is less than the train error
```

# Use tree.control() to grow the tree deeper:
```{r message = FALSE, warning = FALSE}
tree.InterestRate.control = tree.control(nobs = dim(creditDataProcessed.train)[1], mincut=1, minsize = 3, mindev = 0)
#     tree.control() produces control parameters for tree(). 
#     nobs: The number of observations in the training set.
#     mincut: The minimum number of observations to include in a child node. Default: 5.
#     minsize: The smallest allowed node size. Default: 10.
#     mindev: The within-node deviance (RSS) must be at least this times that of the root node for the node to be split. Default: 0.01.
```

```{r message = FALSE, warning = FALSE}
#all predictors
tree.all.control = tree.control(nobs = dim(creditDataProcessed.train)[1], mincut = 5, minsize = 10, mindev = 0.01)

tree.all <- tree(int_rate ~ ., data = creditDataProcessed.train, control=tree.all.control)
summary(tree.all)

plot(tree.all)
text(tree.all, cex=0.75)
```
Evaluation:
The decision tree visualized is modeling the interest rate (int_rate) as the target variable. The tree is making predictions based on two features: revol_util and inq_last_6mths. Here's the interpretation of the tree in the context of predicting interest rates:
-Root Node (revol_util < 51.85): The entire data set is first split based on the revolving utilization rate. If the utilization rate is less than 51.85%, the left branch is followed; otherwise, the right branch is taken. The utilization rate generally represents the amount of credit used relative to the total available credit, and it's a strong indicator in credit scoring because it reflects credit management behavior.
-Left Branch (inq_last_6mths < 0.5): For individuals with a revol_util less than 51.85%:
  --If they have had fewer than 0.5 credit inquiries in the last 6 months (which typically means 0 inquiries), the predicted interest rate is 11.13%.
  --If they have had 0.5 or more inquiries (typically at least 1 inquiry), the predicted interest rate is higher at 13.24%.
-Right Branch (inq_last_6mths < 0.5): For individuals with a revol_util of 51.85% or higher:
  --If they have had fewer than 0.5 inquiries in the last 6 months, the predicted interest rate is 13.35%, slightly higher than the corresponding group with lower revol_util.
  --If they have had 0.5 or more inquiries, the predicted interest rate jumps to 15.24%, which is the highest among all the groups.
These predictions imply a relationship where higher credit utilization (revol_util) and a greater number of recent credit inquiries (inq_last_6mths) are associated with higher interest rates (int_rate). This makes intuitive sense, as both high utilization and frequent inquiries can be indicators of higher credit risk, which lenders might compensate for with higher interest rates.

```{r message = FALSE, warning = FALSE}
# Training error (RMSE)
(tree.all.train.RMSE <- sqrt(summary(tree.all)$dev/summary(tree.all)$df))

# Test error (RMSE)
tree.all.predict <- predict(tree.all, creditDataProcessed.test)
tree.all.predict.dev <- sum((tree.all.predict - creditDataProcessed.test$int_rate)^2)
(tree.all.test.RMSE <- sqrt(tree.all.predict.dev/summary(tree.all)$df)) 

```

# We could play with the tree.control() parameters a bit, or do cross-validation now.
# Instead, we look at cost-complexity pruning. (We will need cross-validation there aslo, to find the best pruning parameter alpha)


##############   5. Pruned Tree   ##########################
# Cost-Complexity Pruning
#   Goal: Prune the tree to avoid high variance and overfitting. 
#   Expected positive effects: 
#     - smaller test errors (due to less overfitting).
#     - higher interpretability (due to smaller trees).
#   Approach: 
#     - Do cross-validation on the training+test set to find the best pruning parameter alpha.
#     - Using the best alpha, grow a pruned tree on the training set.
#     - Evaluate it on the validation set and compare the result with the validation set error from the tree obove.

```{r message = FALSE, warning = FALSE}
# We grow the full tree now
tree.full.control <- tree.control(nobs = dim(creditDataProcessed.train)[1], mincut=1, minsize = 2, mindev = 0.01)
# This set of parameters produce (almost) the full tree ("saturated tree", the tree that fits the data perfectly).
#   Notice:
#     In the tree above (in 4.2), we set the tree.control() parameters to mincut = 5, minsize = 10, mindev = 0.01.
#     These parameters serve as a global threshold for growth, i.e, they stop the growth at a certain point, and the stop critera apply equally to all branches of the tree.
#     In contrast, what we do now is cost complexity pruning: It takes the fully grown tree (no stop critera) and afterwards cuts back each branch.
#     The cutting is *not* the same for all branches, but is done individually - in such a way as to minimize the overall RSS.
tree.full <- tree(int_rate ~ ., creditDataProcessed.train, control = tree.full.control) 
summary(tree.full)
```
```{r message = FALSE, warning = FALSE}
# use cross-validation to find the optimal parameter \alpha for cost-complexity pruning  
set.seed (1)
cv.tree.full = cv.tree(tree.full)
#   Runs a k-fold cross-validation experiment to determin deviance (RSS) 
#   as a function of the cost-complexity parameter alpha.

cv.tree.full
#   $size: number of terminal nodes of each tree
#   $dev: cross-validation deviance (RSS)
#   $k: cost-complexity parameter (alpha)
```
```{r message = FALSE, warning = FALSE}
# Plot the cross-validation deviance as a function of size and alpha
par(mfrow=c(1,2)) # Environment variable to arrange multiple plots in one window: c(1,2)... 1 row, 2 columns 
  plot(cv.tree.full$size, cv.tree.full$dev, type="b", xlab="number of terminal nodes", ylab="deviance") # type="b": plot both, points and lines
  plot(cv.tree.full$k, cv.tree.full$dev, type="b", xlab="alpha", ylab="deviance")
par(mfrow=c(1,1)) # Set back to default.


# Find the tree with smallest CV error
mindev.idx <- which(cv.tree.full$dev == min(cv.tree.full$dev)) 
#   Index with minimal deviance
(best.size <- min(cv.tree.full$size[mindev.idx]))
#   The tree with 8 terminal nodes has lowest cross-validation error.

# Now fit the pruned tree with alpha
tree.pruned <- prune.tree(tree.full, best = best.size)
# prune.tree determines the nested cost-complexity sequence  

summary(tree.pruned)
```
```{r message = FALSE, warning = FALSE}
# Plot the pruned regression tree 
plot(tree.pruned) 
text(tree.pruned, cex=0.75) # cex: set character size to 0.75

# Training error (RMSE)
(tree.pruned.train.RMSE <- sqrt(summary(tree.pruned)$dev/summary(tree.pruned)$df))
#   214 (as compared to 202 in the from 4.2)

# Test error (RMSE)
tree.pruned.predict <- predict(tree.pruned, creditDataProcessed.test)
tree.pruned.predict.dev <- sum((tree.pruned.predict - creditDataProcessed.test$int_rate)^2)
(tree.pruned.test.RMSE <- sqrt(tree.pruned.predict.dev/summary(tree.pruned)$df))
```

##############   6. Bagging (Bootstrap Aggregation)  ##########################
```{r message = FALSE, warning = FALSE}
#install.packages("ranger")
library(ranger)
#   Recall that bagging is just a special case of random forests with m = p.
#   Thus, we can use the function ranger() from the library ranger for bagging also.

# Apply bagging  
set.seed(1)
(tree.bag <- ranger(int_rate ~ ., creditDataProcessed.train, mtry=28, importance = "none", num.trees=500))
#   mtry = 28 means that we use all 28 predictors for each split of the tree - hence, do bagging.
#   importance = TRUE says that variable importance should be assessed.
#   num.trees	... Number of trees to grow. This should not be set to too small a number, to ensure that every 
#             input data point gets predicted at least a few times.
```
Evaluation:
-The model has an R squared of about 0.41, which means approximately 41% of the variance in the interest rate (int_rate) is explained by the model. This isn't particularly high. In many real-world situations, especially with complex datasets, an R squared in this range may still be considered useful.
-The OOB error is a form of cross-validation error, and without knowing the scale of int_rate, it's hard to say whether an MSE of 11.24591 is good or bad. If the interest rate typically varies between 0 and 100, this would be quite high; if it's in the thousands, it's relatively low.
-The mtry parameter is set to 28, which means every split in every tree considered all 28 variables. This is unusual, as typically, mtry is set to a fraction of the total number of variables to ensure that the trees in the forest are de-correlated. You might want to consider using the default value for mtry which is the square root of the number of variables for classification tasks, or one-third of the number of variables for regression tasks.

summary(tree.bag)
  # Lists the attributes of bag.all
```{r message = FALSE, warning = FALSE}
# Training error (RMSE) based on predictions 
tree.bag.predict.train <- predict(tree.bag, data = creditDataProcessed.train)
# Check the structure of the prediction and the original interest rate
str(tree.bag.predict.train)
str(creditDataProcessed.train$int_rate)
# Check for missing values
sum(is.na(tree.bag.predict.train))
sum(is.na(creditDataProcessed.train$int_rate))
# Inspect the first few values
head(tree.bag.predict.train)
head(creditDataProcessed.train$int_rate)
  # We predict the data points of the training set using the ensemble.
  # Remember that these values are the *averaged* predictions of all trees in the ensemble. 
  # Each of the predicted data points was part of the training set of some of the trees.  
  # Thus, the error calculated from these predictions indeed corresponds to a classical "training error".
(tree.bag.train.MSE <- mean((tree.bag.predict.train$predictions - creditDataProcessed.train$int_rate)^2))
(tree.bag.train.RMSE <- sqrt(tree.bag.train.MSE))
#   1.35 - The training error is relatively small. Likely the ensemble is overfitting.
```
# OOB error (RMSE)
# Recall that we actuaklly don't need to do cross validation for bagged trees to get a robust test error estimate.
# The OOB error is the qeuivalent of the cross validation error.

```{r message = FALSE, warning = FALSE}
#Load or prepare your datasets. 
#Example: creditDataProcessed.train and creditDataProcessed.test. 
#Training of the Bagging model
set.seed(1)
tree.bag <- ranger(int_rate ~ ., data = creditDataProcessed.train, mtry = 28, importance = "none", num.trees = 500)
 
# Model summary
summary(tree.bag)
 
# Making predictions on the training data and calculating the training error
tree.bag.predict.train <- predict(tree.bag, data = creditDataProcessed.train)$predictions
tree.bag.train.MSE <- mean((tree.bag.predict.train - creditDataProcessed.train$int_rate)^2)
tree.bag.train.RMSE <- sqrt(tree.bag.train.MSE)
 
# Calculation of the OOB (Out-Of-Bag) error
valid_prediction <- !is.na(tree.bag$predictions)
tree.bag.OOB.MSE <- mean((tree.bag$predictions[valid_prediction] - creditDataProcessed.train$int_rate[valid_prediction])^2, na.rm = TRUE)
tree.bag.OOB.RMSE <- sqrt(tree.bag.OOB.MSE)
 
# Making predictions on the test data and calculating the test error
tree.bag.predict.test <- predict(tree.bag, data = creditDataProcessed.test)$predictions
tree.bag.test.MSE <- mean((tree.bag.predict.test - creditDataProcessed.test$int_rate)^2)
tree.bag.test.RMSE <- sqrt(tree.bag.test.MSE)
 
# Printing the results
cat("Train RMSE:", tree.bag.train.RMSE, "\n")
cat("OOB RMSE:", tree.bag.OOB.RMSE, "\n")
cat("Test RMSE:", tree.bag.test.RMSE, "\n")
```
Evaluation:
-Train RMSE: 1.356286: The RMSE on the training dataset, which is relatively low, suggesting that the model fits the training data well.
-OOB RMSE: 3.353493: The RMSE calculated on the out-of-bag samples, which are the data points that were not included in the bootstrap sample for building each tree. This is higher than the training RMSE, indicating the model may not perform as well on unseen data.
-Test RMSE: 3.336366: The RMSE on a separate test dataset. It is close to the OOB RMSE, which suggests that the OOB error is a good estimate of the test error and that the model's performance is consistent on unseen data.
In summary, the model has been trained on a large dataset and seems to perform well on the training data, but with an increase in error when predicting unseen data (OOB and test data). This could indicate some overfitting to the training data. However, the consistency between OOB and test RMSEs is a good sign, indicating that the model is likely generalizing as expected.

##############   7. Random Forrests   ##########################

# Growing a random forest proceeds in exactly the same way, 
#     except that we use a smaller value of the mtry argument. 
# By default, randomForest() uses 
#     - p/3 variables when building a random forest of regression trees, and 
#     - sqrt(p) variables when building a random forest of classification trees.

# Building a random forest on the same data set using mtry.
```{r message = FALSE, warning = FALSE}
set.seed(1) 
(tree.rf <- randomForest(int_rate ~ ., creditDataProcessed.train, mtry = 28, importance =TRUE, ntree=10)) 
# We took 10 trees because for 10 trees we needed 8 hours and for 500 trees we would have waited for days.
valid_OOB_predictions <- !is.na(tree.rf$predicted)
```

```{r message = FALSE, warning = FALSE}
# Training error (RMSE)
tree.rf.predict.train <- predict(tree.rf, creditDataProcessed.train)
(tree.rf.train.MSE <- mean((tree.rf.predict.train - creditDataProcessed.train$int_rate)^2))
(tree.rf.train.RMSE <- sqrt(tree.rf.train.MSE))

# OOB error (RMSE)
(tree.rf.OOB.MSE <- mean((tree.rf$predicted[valid_OOB_predictions] - creditDataProcessed.train$int_rate[valid_OOB_predictions])^2, na.rm = TRUE))
(tree.rf.OOB.RMSE <- sqrt(tree.rf.OOB.MSE))

# Test error (RMSE)
tree.rf.predict.test <- predict(tree.rf, creditDataProcessed.train)
(tree.rf.test.MSE <- mean((tree.rf.predict.test - creditDataProcessed.train$int_rate)^2))
(tree.rf.test.RMSE <- sqrt(tree.rf.test.MSE))

# Variable Importance 
importance(tree.rf)
sort(importance(tree.rf)[,1], decreasing = T)
(varImpPlot(tree.rf))

# Optimizing the ntree parameter
plot(tree.rf)
```


##############   Boosting   ##########################
```{r message = FALSE, warning = FALSE}
#   We use the gbm() function from the gbm library 
library(gbm)
# Perform boosting on the training data set, treating this as a regression problem. 
set.seed (1)
```

#Preprocessing for Boosting
```{r message = FALSE, warning = FALSE}
creditDataProcessedFactorised <- creditDataProcessed

# 1. discover the  frequency
freq_table <- table(creditDataProcessedFactorised$Combo)
# 2. Filtering the categories which occur just once
rare_categories <- names(freq_table[freq_table == 1])
# 3. Update the 'Combo'-Variable
# Here we are removing the rarely occuring categories and replacing them ith NA
creditDataProcessedFactorised$Combo <- ifelse(creditDataProcessedFactorised$Combo %in% rare_categories, "Other", creditDataProcessedFactorised$Combo)
creditDataProcessedFactorised$Combo <- factor(creditDataProcessedFactorised$Combo)


cols_to_factorise <- c("home_ownership", "verification_status", "purpose","title","zip_code","addr_state","earliest_cr_line","initial_list_status","last_credit_pull_d","application_type","Combo" )  
creditDataProcessedFactorised[cols_to_factorise] <- lapply(creditDataProcessedFactorised[cols_to_factorise], as.factor)

train_indx2 <- sample(1:nrow(creditDataProcessedFactorised), 0.7*nrow(creditDataProcessedFactorised))
creditDataProcessedFactorised.train <- creditDataProcessedFactorised[train_indx2,]
creditDataProcessedFactorised.test <- creditDataProcessedFactorised[-train_indx2,]

```


```{r message = FALSE, warning = FALSE}
(tree.boost <- gbm(int_rate ~ ., creditDataProcessedFactorised.train, distribution="gaussian", n.trees=100, interaction.depth = 1, shrinkage = 0.001, verbose = F))
# distribution = "gaussian" ... refers to a regression problem. 
# n.trees	                  ... Integer specifying the total number of trees to fit (number of iterations). Default is 100.
# interaction.depth         ... refers to the maximum depth of variable interactions. 
#                               A value of 1 implies an additive model, a value of 2 implies a model with up to 
#                               2-way interactions, etc. Default is 1.
# shrinkag                  ... a shrinkage parameter, also known as the learning rate or step-size reduction; 
#                               0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees. Default is 0.1.

# Training error (RMSE)
tree.boost.predict.train <- predict(tree.boost, creditDataProcessedFactorised.train)
(tree.boost.train.MSE <- mean((tree.boost.predict.train - creditDataProcessedFactorised.train$int_rate)^2))
(tree.boost.train.RMSE <- sqrt(tree.boost.train.MSE))
#   0.002
#   Our boosted ensemble seems to overfit! - Lets check the test error.

# Test error (RMSE)
tree.boost.predict.test <- predict(tree.boost, creditDataProcessedFactorised.test)
(tree.boost.test.MSE <- mean((tree.boost.predict.test - creditDataProcessedFactorised.test$int_rate)^2))
(tree.boost.test.RMSE <- sqrt(tree.boost.test.MSE))
#   373 
#   Definitely overfitting...

# Variable importance
summary(tree.boost)
# outputs the variable importance as a table and a plot
#   rel.inf ... relative variable importance ("permutation importance"), same as %IncMSE used for random forests.
#   It measures the importance of a predictor by the average increase in prediction error when the values  
#   of a given predictor are shuffled (permuted). The values are normalized so that they add up to 100%.
#   If you dont see a variable in your plot, expand the plot window.
#  We have different variables on top here...



##############   Evaluation Sum-Up  ##############   

# Now compare the different errors:
Tree          <- c(tree.all.train.RMSE, tree.all.test.RMSE)
prunedTree    <- c(tree.pruned.train.RMSE, tree.pruned.test.RMSE)
baggedTrees   <- c(tree.bag.train.RMSE, tree.bag.OOB.RMSE)
randomForest  <- c(tree.rf.train.RMSE, tree.rf.OOB.RMSE)
boostedTrees  <- c(tree.boost.train.RMSE, tree.boost.test.RMSE)

error_matrix <- data.frame(Tree, prunedTree, baggedTrees, randomForest, boostedTrees)
row.names(error_matrix) <- c("train", "test/OOB")
error_matrix
```